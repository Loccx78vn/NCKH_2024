---
title: "Moderation_Part1"
author: "Lộc"
date: "2024-02-28"
output: 
  html_document:
    theme: flatly
    toc: true
    number_sections : true
    toc_float: 
      collapsed: true
    toc-location: left
    code_folding: hide
---

# Input:

## Prepare the dataset:

```{r}
#| echo: false
#Call packages
pacman::p_load(rio,
               here,
               janitor,
               tidyverse,
               dplyr,
               magrittr,
               ggplot2,
               purrr,
               lubridate,
               mice,
               plotly)

#Data:
library(readxl)
df<- read_excel("C:/Users/locca/Downloads/Data_Rác thải điện tử.xlsx", 
    sheet = "Đã mã hóa")
```

Likert data is a type of ordinal data used in surveys and questionnaires to measure attitudes, opinions, or perceptions. Named after psychologist Rensis Likert, who developed the scale, it typically involves respondents indicating their level of agreement or disagreement with a series of statements on a scale.

**Key Features:**

1.  **Scale Structure:** Likert scales often use a 5-point or 7-point scale, ranging from strong disagreement to strong agreement. For example, a 5-point scale might include options like "Strongly Disagree," "Disagree," "Neutral," "Agree," and "Strongly Agree."

2.  **Ordinal Nature:** The data collected are ordinal, meaning they represent ordered categories, but the intervals between them are not necessarily equal. For example, the difference between "Agree" and "Strongly Agree" isn't quantifiable in the same way as numerical data.

3.  **Data Analysis:** Likert data is often analyzed using descriptive statistics like means and standard deviations, as well as inferential statistics to explore relationships or differences between groups. While some analyses treat the data as interval-level for convenience, it's crucial to remember its ordinal nature.

4.  **Applications:** Likert data is commonly used in social science research, customer satisfaction surveys, and employee feedback forms to gauge attitudes and opinions.

```{r}
#| echo: false
df_new <-df %>% 
  mutate(SAFETY = (SAFETY1+SAFETY2+SAFETY3+SAFETY4)/4,
         OFFER = (OFFER1+OFFER2+OFFER3+OFFER4)/4,
         AWARENESS = (AWARENESS1+AWARENESS2+AWARENESS3+AWARENESS4)/4,
         SUPPLYINPUT =(SUPPLYINPUT1+SUPPLYINPUT2+SUPPLYINPUT3+SUPPLYINPUT4)/4,
         REVERSE_DECISION = (REVERSE_DECISION1+REVERSE_DECISION2+REVERSE_DECISION3+REVERSE_DECISION4)/4)

m<-df %>% select(contains(c("SAFETY",
                            "OFFER",
                            "SUPPLY",
                            "AWARE",
                            "REVERSE")))
```

### Likert chart:

Overall, Likert scales are a popular and useful tool for capturing subjective information, though the ordinal nature of the data requires careful consideration in analysis and interpretation.

```{r Likert chart}
#===========================
# Simulate data for ploting 
#===========================

responses <- c("Hoàn toàn không đồng ý", 
               "Không đồng ý", 
               "Bình thường", 
               "Đồng ý", 
               "Hoàn toàn đồng ý")

size <- nrow(df_new)

brand <- as.vector(unlist(purrr::map(.x = unique(names(m)),
                                     .f = ~rep(.x,size)))
)

prob = c(sum(df_new$SAFETY1 == 1)/size,
         sum(df_new$SAFETY1 == 2)/size,
         sum(df_new$SAFETY1 == 3)/size,
         sum(df_new$SAFETY1 == 4)/size,
         sum(df_new$SAFETY1 == 5)/size)

cus_res <- c(
      ### For SAFETY variable:
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SAFETY1 == 1)/size,
                             sum(df_new$SAFETY1 == 2)/size,
                              sum(df_new$SAFETY1 == 3)/size,
                              sum(df_new$SAFETY1 == 4)/size,
                              sum(df_new$SAFETY1 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SAFETY2 == 1)/size,
                             sum(df_new$SAFETY2 == 2)/size,
                              sum(df_new$SAFETY2 == 3)/size,
                              sum(df_new$SAFETY2 == 4)/size,
                              sum(df_new$SAFETY2 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SAFETY3 == 1)/size,
                             sum(df_new$SAFETY3 == 2)/size,
                              sum(df_new$SAFETY3 == 3)/size,
                              sum(df_new$SAFETY3 == 4)/size,
                              sum(df_new$SAFETY3 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SAFETY4 == 1)/size,
                             sum(df_new$SAFETY4 == 2)/size,
                              sum(df_new$SAFETY4 == 3)/size,
                              sum(df_new$SAFETY4 == 4)/size,
                              sum(df_new$SAFETY4 == 5)/size)),

     ### For OFFER variable:
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$OFFER1 == 1)/size,
                             sum(df_new$OFFER1 == 2)/size,
                              sum(df_new$OFFER1 == 3)/size,
                              sum(df_new$OFFER1 == 4)/size,
                              sum(df_new$OFFER1 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$OFFER2 == 1)/size,
                             sum(df_new$OFFER2 == 2)/size,
                              sum(df_new$OFFER2 == 3)/size,
                              sum(df_new$OFFER2 == 4)/size,
                              sum(df_new$OFFER2 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$OFFER3 == 1)/size,
                             sum(df_new$OFFER3 == 2)/size,
                              sum(df_new$OFFER3 == 3)/size,
                              sum(df_new$OFFER3 == 4)/size,
                              sum(df_new$OFFER3 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$OFFER4 == 1)/size,
                             sum(df_new$OFFER4 == 2)/size,
                              sum(df_new$OFFER4 == 3)/size,
                              sum(df_new$OFFER4 == 4)/size,
                              sum(df_new$OFFER4 == 5)/size)),
      ### For SUPPLYINPUT variable:
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SUPPLYINPUT1 == 1)/size,
                             sum(df_new$SUPPLYINPUT1 == 2)/size,
                              sum(df_new$SUPPLYINPUT1 == 3)/size,
                              sum(df_new$SUPPLYINPUT1 == 4)/size,
                              sum(df_new$SUPPLYINPUT1 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SUPPLYINPUT2 == 1)/size,
                             sum(df_new$SUPPLYINPUT2 == 2)/size,
                              sum(df_new$SUPPLYINPUT2 == 3)/size,
                              sum(df_new$SUPPLYINPUT2 == 4)/size,
                              sum(df_new$SUPPLYINPUT2 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SUPPLYINPUT3 == 1)/size,
                             sum(df_new$SUPPLYINPUT3 == 2)/size,
                              sum(df_new$SUPPLYINPUT3 == 3)/size,
                              sum(df_new$SUPPLYINPUT3 == 4)/size,
                              sum(df_new$SUPPLYINPUT3 == 5)/size)),
              sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$SUPPLYINPUT4 == 1)/size,
                             sum(df_new$SUPPLYINPUT4 == 2)/size,
                              sum(df_new$SUPPLYINPUT4 == 3)/size,
                              sum(df_new$SUPPLYINPUT4 == 4)/size,
                              sum(df_new$SUPPLYINPUT4 == 5)/size)),
       ### For AWARENESS variable:
            sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$AWARENESS1 == 1)/size,
                             sum(df_new$AWARENESS1 == 2)/size,
                              sum(df_new$AWARENESS1 == 3)/size,
                              sum(df_new$AWARENESS1 == 4)/size,
                              sum(df_new$AWARENESS1 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$AWARENESS2 == 1)/size,
                             sum(df_new$AWARENESS2 == 2)/size,
                              sum(df_new$AWARENESS2 == 3)/size,
                              sum(df_new$AWARENESS2 == 4)/size,
                              sum(df_new$AWARENESS2 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$AWARENESS3 == 1)/size,
                             sum(df_new$AWARENESS3 == 2)/size,
                              sum(df_new$AWARENESS3 == 3)/size,
                              sum(df_new$AWARENESS3 == 4)/size,
                              sum(df_new$AWARENESS3 == 5)/size)),
              sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$AWARENESS4 == 1)/size,
                             sum(df_new$AWARENESS4 == 2)/size,
                              sum(df_new$AWARENESS4 == 3)/size,
                              sum(df_new$AWARENESS4 == 4)/size,
                              sum(df_new$AWARENESS4 == 5)/size)),

      ### For AWARENESS variable:
            sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$REVERSE_DECISION1 == 1)/size,
                             sum(df_new$REVERSE_DECISION1 == 2)/size,
                              sum(df_new$REVERSE_DECISION1 == 3)/size,
                              sum(df_new$REVERSE_DECISION1 == 4)/size,
                              sum(df_new$REVERSE_DECISION1 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$REVERSE_DECISION2 == 1)/size,
                             sum(df_new$REVERSE_DECISION2 == 2)/size,
                              sum(df_new$REVERSE_DECISION2 == 3)/size,
                              sum(df_new$REVERSE_DECISION2 == 4)/size,
                              sum(df_new$REVERSE_DECISION2 == 5)/size)),
             sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$REVERSE_DECISION3 == 1)/size,
                             sum(df_new$REVERSE_DECISION3 == 2)/size,
                              sum(df_new$REVERSE_DECISION3 == 3)/size,
                              sum(df_new$REVERSE_DECISION3 == 4)/size,
                              sum(df_new$REVERSE_DECISION3 == 5)/size)),
              sample(responses, 
                    size = size, 
                    replace = TRUE, 
                    prob = c(sum(df_new$REVERSE_DECISION4 == 1)/size,
                             sum(df_new$REVERSE_DECISION4 == 2)/size,
                              sum(df_new$REVERSE_DECISION4 == 3)/size,
                              sum(df_new$REVERSE_DECISION4 == 4)/size,
                              sum(df_new$REVERSE_DECISION4 == 5)/size))
)
```

```{r,warning = F}
#===========================
# Prepare data for ploting
#===========================

data_cus <- tibble(brand = brand, cus_res = cus_res)

data_cus %>% 
  group_by(brand, cus_res) %>% 
  count() %>% 
  ungroup() %>% 
  group_by(brand) %>% 
  mutate(percent = 100*n / sum(n)) %>% 
  mutate(percent = round(percent, 0)) %>% 
  mutate(bar_text = paste0(percent, "%")) %>% 
  ungroup() -> df_for_ploting

df_for_ploting %>% 
  filter(cus_res == responses[5]) %>% 
  arrange(percent) %>% 
  pull(brand) -> order_y

df_for_ploting %>% 
  mutate(brand = factor(brand, levels = order_y)) %>% 
  mutate(cus_res = factor(cus_res, levels = responses[5:1])) -> df_odered

#---------------------
# Data Vis: Version 1
#---------------------

## Select Font for the graph: 

col_dislike_alot <- "#e36c33"

col_dislike <- "#edad88"

col_neutral <- "#c7cdd1"

col_like <- "#829cb2"

col_like_alot <- "#3e6487"

  
library(showtext)
my_font <- "Roboto Condensed"
font_add_google(name = my_font, family = my_font)

showtext_auto()

theme_set(theme_minimal())

df_odered %>% 
  ggplot(aes(y = brand, x = percent, fill = cus_res)) + 
  geom_col(width = 0.8, position = "fill") + 
  theme(legend.position = "top") + 
  theme(plot.margin = unit(rep(0.7, 4), "cm")) +
  scale_fill_manual(values = c('Hoàn toàn đồng ý' = col_like_alot,
                               'Đồng ý'= col_like, 
                               'Bình thường' = col_neutral,
                               'Không đồng ý'  = col_dislike, 
                               'Hoàn toàn không đồng ý' = col_dislike_alot),
                   guide = guide_legend(reverse = TRUE)) +
  theme(text = element_text(family = my_font)) + 
  theme(legend.title = element_blank()) + 
  theme(legend.text = element_text(size = 11, family = my_font, color = "grey10")) + 
  theme(legend.key.height = unit(0.35, "cm")) +  
  theme(legend.key.width = unit(0.27*3, "cm")) + 
  theme(axis.title = element_blank()) + 
  theme(panel.grid.minor = element_blank()) + 
  theme(panel.grid.major.x = element_line(color = "grey70", size = 0.8)) + 
  scale_x_continuous(expand = c(0, 0), labels = paste0(seq(0, 100, 25), "%")) + 
  scale_y_discrete(expand = c(0, 0)) + 
  theme(axis.text = element_text(color = "grey30", size = 11, family = my_font)) + 
  theme(plot.title = ggtext::element_markdown(size = 16, face = "bold")) + 
  theme(plot.caption = element_text(size = 10.5, color = "grey40", vjust = -1.5, hjust = 0)) + 
  theme(plot.subtitle = element_text(size = 11.5, color = "grey10")) + 
  theme(plot.title.position = "plot") +  
  theme(plot.caption.position = "plot")->gg1
gg1
```

### The distribution plot of likert scales:

In other hand, we can use a distribution plot of Likert data visualizes how responses are distributed across the different scale points. Given the ordinal nature of Likert data, such a plot helps to understand the frequency or proportion of responses for each category on the scale.

Steps to Create a Distribution Plot for Likert Data:

1.    **Collect Data**: Gather your Likert scale responses from the survey or questionnaire.

2.    **Tabulate Responses**: Count the number of responses for each Likert scale point (e.g., "Strongly Disagree" to "Strongly Agree").

3.    **Choose a Visualization Tool**: You can use various tools like Excel, Google Sheets, or statistical software (e.g., R, Python's Matplotlib/Seaborn) to create the plot.

4.    **Plot the Data**: you can consider between **Bar Chart** - where each bar represents the frequency or percentage of responses for each scale point or Pie Chart which shows the proportion of responses for each scale point. If you are dealing with multiple groups or categories, Stacked Bar Chart maybe useful where bars are segmented to show the distribution across the Likert scale for each group.

```{r}

m_new <-m %>% pivot_longer(cols = everything(),
                           names_to = "variable",
                           values_to = "value")

ggplot()+
  ggridges::geom_density_ridges(data  = m_new, 
                                aes(x  = value,
                                    y  = as.factor(variable),
                                    fill = as.factor(variable),
                                    height = ..density..), 
                                scale = 3, 
                                alpha = .6) +
  scale_x_continuous(limits = c(0,6))+
  geom_vline(xintercept = 3, col = "red",size = 2)+
  geom_point(data = data.frame(list(variable = unique(m_new$variable), value = colMeans(m))),
             aes(x = value, 
                 y = as.factor(variable)),
             size = 3, 
             col  = "blue")+
  theme(legend.position="none")+
  viridis::scale_fill_viridis(discrete = TRUE)+
  labs(x        = "Gía trị Likert",
       y        = "Biến trong thang đo")
       
```

### The Correlation coefficients analyst:

```{r}
library(corrplot)
library(grDevices)
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot.mixed(cor(df_new %>%
         select(c("SAFETY",
                  "SUPPLYINPUT",
                  "OFFER",
                  "AWARENESS",
                  "REVERSE_DECISION"))),
               tl.cex = 0.6,
               number.cex = 1.5,
               lower = "number", 
               upper = "circle",
               tl.col = "black")
```

# Output:

In this chapter, I will use 2 model: **Linear regression model** and **SEM (Structural equation modeling)** to fit this data.

## Linear regression analyst:

Linear regression is a statistical method used to model and analyze the relationship between a dependent variable and one or more independent variables. It assumes that the relationship between the variables can be approximated by a straight line.

**Key Concepts of Linear Regression:**

1. **Simple Linear Regression:**
  - **Purpose:** To model the relationship between a single independent variable (predictor) and a dependent variable (outcome).
- **Equation:** The relationship is expressed by the linear equation:
  \[
    y = \beta_0 + \beta_1 x + \epsilon
    \]
where:
  - \( y \) is the dependent variable.
- \( x \) is the independent variable.
- \( \beta_0 \) is the y-intercept (constant term).
- \( \beta_1 \) is the slope of the line (coefficient of the independent variable).
- \( \epsilon \) is the error term (residuals).

2. **Multiple Linear Regression:**
  - **Purpose:** To model the relationship between two or more independent variables and a dependent variable.
- **Equation:** The relationship is expressed by:
  \[
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon
    \]
where \( x_1, x_2, \ldots, x_n \) are the independent variables and \( \beta_1, \beta_2, \ldots, \beta_n \) are their respective coefficients.

3. **Assumptions:**
  - **Linearity:** The relationship between the dependent and independent variables is linear.
- **Independence:** Observations are independent of each other.
- **Homoscedasticity:** The variance of residuals is constant across all levels of the independent variables.
- **Normality:** Residuals (errors) of the model are normally distributed.

4. **Objective:**
  - **Fit the Model:** Determine the best-fitting line through the data by minimizing the sum of squared residuals (differences between observed and predicted values).
- **Prediction:** Use the model to make predictions about the dependent variable based on new values of the independent variables.

5. **Evaluation Metrics:**
  - **R-squared (\( R^2 \)):** Measures the proportion of variance in the dependent variable that is predictable from the independent variables.
- **Adjusted R-squared:** Adjusted for the number of predictors in the model, useful for multiple regression.
- **Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):** Measure the average squared difference between observed and predicted values.
- **p-values:** Test the statistical significance of each coefficient in the model.

### Preparing:

```{r, warning = F}
#Recode the value of AGE variable:
df_new<-df_new %>% 
  mutate(AGE = recode(AGE,
                      "Dưới 18 tuổi" = "A",
                      "Từ 19 đến 30 tuổi" = "B",
                      "Từ 31 đến 50 tuổi" = "C",
                      "Từ 50 tuổi trở lên" = "D"))

#Show the amount and percentage of group in AGE:
tabyl(df_new$AGE)

#Convert the RETURN_EW into factor class:
df_new$RETURN_EW<-as.factor(df_new$RETURN_EW)
```

### Anova for comparing between groups:

```{r}
summary(reg1<-aov(AWARENESS~AGE,data = df_new))

plot(TukeyHSD(reg1,conf.level=0.95), las=1 , col="brown")
```

### Create dummy variables:

A dummy variable is a numerical variable used in statistical modeling to represent categorical data, often for the purpose of including categorical variables in regression analysis. Dummy variables allow you to incorporate categorical predictors into models that require numerical input.About **Key Concepts of Dummy Variables:**:

1. **Purpose:**
  - Dummy variables are used to encode categorical variables into a format that can be used in statistical and machine learning models. This encoding helps to quantify categories in a way that models can interpret.

2. **Creation:**
  - For a categorical variable with \( k \) categories, you create \( k-1 \) dummy variables. This is because using \( k \) dummy variables would create multicollinearity issues (known as the dummy variable trap).
- Each dummy variable represents one category of the categorical variable, with values typically coded as 0 or 1. A value of 1 indicates the presence of a category, while 0 indicates its absence.

3. **Example:**
  - Suppose you have a categorical variable "Color" with three categories: Red, Blue, and Green. To include this in a regression model, you would create two dummy variables (since \( 3-1 = 2 \)):
  
  - \( \text{Dummy}_1 \) (for Blue): 
  - 1 if the color is Blue
- 0 otherwise
- \( \text{Dummy}_2 \) (for Green): 
  - 1 if the color is Green
- 0 otherwise

- Red is the reference category (when both dummy variables are 0).

4. **Interpretation:**
  - The coefficients of the dummy variables in a regression model represent the difference in the dependent variable relative to the reference category. For instance, if you are predicting sales and "Blue" has a coefficient of 500 compared to the reference "Red," it suggests that the sales for Blue are on average 500 units higher than for Red.

5. **Applications:**
  - Dummy variables are used in linear regression, logistic regression, ANOVA (Analysis of Variance), and various machine learning models to include categorical features.
- They are also employed in time series analysis to account for seasonality or other categorical effects.

**Example in Linear Regression:**

Suppose you want to include the "Color" variable in a linear regression model predicting price. The regression model might look like this:
  
  \[
    \text{Price} = \beta_0 + \beta_1 \text{Dummy}_1 + \beta_2 \text{Dummy}_2 + \epsilon
    \]

where:
  - \( \text{Dummy}_1 \) represents Blue,
- \( \text{Dummy}_2 \) represents Green,
- \( \beta_0 \) is the intercept (Price for the reference category Red),
- \( \beta_1 \) is the effect of being Blue relative to Red,
- \( \beta_2 \) is the effect of being Green relative to Red.

If \( \beta_1 = 100 \) and \( \beta_2 = 150 \), it means the price for Blue items is on average $100 higher and for Green items is $150 higher than Red items.

```{r}
#Create a matrix contain 3 dummy variables:
res <- model.matrix(~AGE, data = df_new)
head(res[, -1])

#Add dummy cols into dataset
df_new<-df_new %>% 
  mutate(A_B = res[,2],
         A_C = res[,3],
         A_D = res[,4])

#Build model:
model <- '
  # Direct effects
  AWARENESS ~ a*A_B  +  b*A_C + c*A_D
  REVERSE_DECISION ~ d*AWARENESS

  # Indirect effect:
  indirect := d*(a + b + c)
  
   # Total effect:
  total := indirect'

library(lavaan)
reg4 <- sem(model,estimator = "ML",data = df_new)
##Summarize the results
summary(reg4, 
        standardized = TRUE, 
        fit.measures = TRUE)
```

### Choose the best models:

```{r}
library(BMA)
#Define the dependent and independent variables:
yvar<-df_new[,("REVERSE_DECISION")]

xvars<-df_new[,c(          "SAFETY","OFFER","AWARENESS","SUPPLYINPUT")]

bma = bicreg(xvars, yvar)
#Summary and plot the results:
summary(bma)
imageplot.bma(bma)
```

### Define the final linear model:

```{r,warning = F}
#Assume X3 is a moderation variable:
library(jtools)
summ(reg2<-lm(REVERSE_DECISION ~ AWARENESS*AGE,
              data = df_new))
```

### Define the SEM model:

```{r,warning = F}
mediation_model <- '
  AWARENESS ~ SAFETY  +  OFFER +  SUPPLYINPUT
  REVERSE_DECISION ~ AWARENESS + SAFETY + OFFER'

reg3<-sem(mediation_model,
          data = df_new, 
          group = "MAJOR",
          group.equal=c("thresholds", "loadings"))

summary(reg3,
        standardized = TRUE, 
        fit.measures = TRUE)
```

### Some effected plots:

```{r,warning = F}
library(jtools)
interactions::interact_plot(reg2, 
                            pred = "AWARENESS", 
                            modx = "AGE")

effect_plot(reg2, 
            pred = AWARENESS, 
            interval = TRUE, 
            plot.points = TRUE, 
            jitter = 0.05)

plot_summs(reg2)
```

## Check the model assumptions:

### Test the varaince hypothesis:

```{r}
#Kiểm định phương sai:
knitr::kable(anova(reg2), digits = 3)
```

### Test the variance hypothesis:

```{r}
#Kiểm định phương sai:
library(car)
ncvTest(reg2)
spreadLevelPlot(reg2)

## -->Vì p < 0 nên giả định phương sai không đổi bị phủ định. (Thường kết quả kiểm định này khá nhạy)
```

### Test for linearity hypothesis:

```{r}
# Kiểm định tính tuyến tính
## Gỉa định mối tương quan là tuyến tính: 
crPlots(reg2,
        col.lines = c("blue", "red"),
        ylab = "Component + Residual")
```

### Test for Autocorrelated Errors hypothesis:

SỬ dụng phương pháp Durbin-Watson Test để kiểm định sự độc lập giữa các biến.

-   Giả thuyết H0: Không có hiện tượng tự tương quan (Autocorrelation) trong mô hình.

-   Gỉa thuyết H1: Có hiện tượng tự tương quan (Autocorrelation) trong mô hình.

Vì p \> 0 nên chấp nhận giả thuyết H0, bác bỏ H1

```{r}
## Computes residual autocorrelations and generalized Durbin-Watson statistics and their bootstrapped p-values.
durbinWatsonTest(reg2)
```

### Test for the Outlier hypothesis

Sử dụng phương pháp Bonferroni test để kiếm tra các outliers.

Biểu đồ cho thấy các điểm có label là những điểm outliers. Ví dụ trong hình dưới đây điểm có số id là **170** là outlier.

```{r}
## Reports the Bonferroni p-values for testing each observation in turn to be a mean-shift outlier, based Studentized residuals in linear (t-tests), generalized linear models (normal tests), and linear mixed models.
outlierTest(reg2)
library(car)
leveragePlots(reg2,
              lwd = 3)
```

### Test for importance affect in model

```{r,warning =F}
#Evaluate the relative importance:
##Use traditional method:
library(relaimpo)
metrics <-calc.relimp(reg2,
                      type = c("lmg"))

(x<-data.frame(name = c("SAFETY","OFFER","AWARENESS","RETURN_EW"),
              value = metrics@lmg) %>% 
  mutate(name = fct_reorder(name, 
                            value)) %>%
  ggplot( aes(x=name, 
              y=value)) +
    geom_bar(stat="identity", 
             fill="#f68060", 
             alpha=.6, 
             width=.4) +
    coord_flip() +
    xlab("") +
    theme_bw())

##Using bootstrap:
boot<-boot.relimp(reg2,
                  b = 1000,
                  type = c("lmg"),
                  fixed = F)
booteval.relimp(boot,
                typesel = c("lmg"),
                level = 0.9,
                bty = "perc",
                nodiff = T)
```
