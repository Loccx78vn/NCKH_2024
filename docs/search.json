[
  {
    "objectID": "NCKH_final.html",
    "href": "NCKH_final.html",
    "title": "Introduction",
    "section": "",
    "text": "I cannot share the data so you can see figures below illustrated how data looks like.\n\n\nLikert data is a type of ordinal data used in surveys and questionnaires to measure attitudes, opinions, or perceptions. Named after psychologist Rensis Likert, who developed the scale, it typically involves respondents indicating their level of agreement or disagreement with a series of statements on a scale.\nKey Features:\n\nScale Structure: Likert scales often use a 5-point or 7-point scale, ranging from strong disagreement to strong agreement. For example, a 5-point scale might include options like “Strongly Disagree,” “Disagree,” “Neutral,” “Agree,” and “Strongly Agree.”\nOrdinal Nature: The data collected are ordinal, meaning they represent ordered categories, but the intervals between them are not necessarily equal. For example, the difference between “Agree” and “Strongly Agree” isn’t quantifiable in the same way as numerical data.\nData Analysis: Likert data is often analyzed using descriptive statistics like means and standard deviations, as well as inferential statistics to explore relationships or differences between groups. While some analyses treat the data as interval-level for convenience, it’s crucial to remember its ordinal nature.\nApplications: Likert data is commonly used in social science research, customer satisfaction surveys, and employee feedback forms to gauge attitudes and opinions.\n\n\n\nCode\ndf_new &lt;-df %&gt;% \n  mutate(SAFETY = (SAFETY1+SAFETY2+SAFETY3+SAFETY4)/4,\n         OFFER = (OFFER1+OFFER2+OFFER3+OFFER4)/4,\n         AWARENESS = (AWARENESS1+AWARENESS2+AWARENESS3+AWARENESS4)/4,\n         SUPPLYINPUT =(SUPPLYINPUT1+SUPPLYINPUT2+SUPPLYINPUT3+SUPPLYINPUT4)/4,\n         REVERSE_DECISION = (REVERSE_DECISION1+REVERSE_DECISION2+REVERSE_DECISION3+REVERSE_DECISION4)/4)\n\nm&lt;-df %&gt;% select(contains(c(\"SAFETY\",\n                            \"OFFER\",\n                            \"SUPPLY\",\n                            \"AWARE\",\n                            \"REVERSE\")))\n\ndf_new&lt;-df_new %&gt;% \n  select(-names(m))\n\n  \n##Convert categorical cols in df_new to factor class:\ncols &lt;- c(names(df %&gt;%  select (-c(names(m)))))\ndf_new[cols]&lt;-lapply(df_new[cols],factor)\n\n##Check again:\nglimpse(df_new)\n\n\nRows: 208\nColumns: 13\n$ SEX              &lt;fct&gt; Nam, Nữ, Nam, Nam, Nữ, Nam, Nữ, Nam, Nữ, Nữ, Nữ, Nam,…\n$ AGE              &lt;fct&gt; Từ 19 đến 30 tuổi, Từ 19 đến 30 tuổi, Từ 19 đến 30 tu…\n$ MAJOR            &lt;fct&gt; Trí thức, Học sinh - Sinh viên, Học sinh - Sinh viên,…\n$ STATUS           &lt;fct&gt; Đã có gia đình, Độc thân, Độc thân, Đang có người yêu…\n$ RETURN_EW        &lt;fct&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PRE_METHOD       &lt;fct&gt; Vứt rác, Bán ve chai, Vứt rác, Bán ve chai, Bán ve ch…\n$ JOIN_HAND        &lt;fct&gt; 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,…\n$ PRIOR            &lt;fct&gt; Hoàn trả doanh nghiệp, Hoàn trả doanh nghiệp, Không b…\n$ SAFETY           &lt;dbl&gt; 4.00, 3.75, 3.75, 3.25, 3.00, 5.00, 4.00, 4.00, 3.50,…\n$ OFFER            &lt;dbl&gt; 3.00, 3.50, 3.25, 3.75, 3.00, 5.00, 4.00, 4.00, 4.00,…\n$ AWARENESS        &lt;dbl&gt; 3.00, 3.75, 3.25, 3.50, 3.00, 5.00, 3.50, 4.00, 4.75,…\n$ SUPPLYINPUT      &lt;dbl&gt; 3.00, 4.50, 3.75, 4.00, 3.00, 5.00, 3.25, 4.00, 4.75,…\n$ REVERSE_DECISION &lt;dbl&gt; 3.00, 2.50, 3.25, 2.75, 3.00, 5.00, 4.00, 4.00, 4.75,…\n\n\n\n\nOverall, Likert scales are a popular and useful tool for capturing subjective information, though the ordinal nature of the data requires careful consideration in analysis and interpretation.\n\n\n\n\n\n\n\n\nIn other hand, we can use a distribution plot of Likert data visualizes how responses are distributed across the different scale points. Given the ordinal nature of Likert data, such a plot helps to understand the frequency or proportion of responses for each category on the scale.\nSteps to Create a Distribution Plot for Likert Data:\n\nCollect Data: Gather your Likert scale responses from the survey or questionnaire.\nTabulate Responses: Count the number of responses for each Likert scale point (e.g., “Strongly Disagree” to “Strongly Agree”).\nChoose a Visualization Tool: You can use various tools like Excel, Google Sheets, or statistical software (e.g., R, Python’s Matplotlib/Seaborn) to create the plot.\nPlot the Data: you can consider between Bar Chart - where each bar represents the frequency or percentage of responses for each scale point or Pie Chart which shows the proportion of responses for each scale point. If you are dealing with multiple groups or categories, Stacked Bar Chart maybe useful where bars are segmented to show the distribution across the Likert scale for each group.\n\n\n\n\n\n\n\n\n\nA correlation test is used to determine the strength and direction of the relationship between two continuous variables. It assesses whether changes in one variable are associated with changes in another variable."
  },
  {
    "objectID": "NCKH_final.html#prepare-the-dataset",
    "href": "NCKH_final.html#prepare-the-dataset",
    "title": "Introduction",
    "section": "",
    "text": "Likert data is a type of ordinal data used in surveys and questionnaires to measure attitudes, opinions, or perceptions. Named after psychologist Rensis Likert, who developed the scale, it typically involves respondents indicating their level of agreement or disagreement with a series of statements on a scale.\nKey Features:\n\nScale Structure: Likert scales often use a 5-point or 7-point scale, ranging from strong disagreement to strong agreement. For example, a 5-point scale might include options like “Strongly Disagree,” “Disagree,” “Neutral,” “Agree,” and “Strongly Agree.”\nOrdinal Nature: The data collected are ordinal, meaning they represent ordered categories, but the intervals between them are not necessarily equal. For example, the difference between “Agree” and “Strongly Agree” isn’t quantifiable in the same way as numerical data.\nData Analysis: Likert data is often analyzed using descriptive statistics like means and standard deviations, as well as inferential statistics to explore relationships or differences between groups. While some analyses treat the data as interval-level for convenience, it’s crucial to remember its ordinal nature.\nApplications: Likert data is commonly used in social science research, customer satisfaction surveys, and employee feedback forms to gauge attitudes and opinions.\n\n\n\nCode\ndf_new &lt;-df %&gt;% \n  mutate(SAFETY = (SAFETY1+SAFETY2+SAFETY3+SAFETY4)/4,\n         OFFER = (OFFER1+OFFER2+OFFER3+OFFER4)/4,\n         AWARENESS = (AWARENESS1+AWARENESS2+AWARENESS3+AWARENESS4)/4,\n         SUPPLYINPUT =(SUPPLYINPUT1+SUPPLYINPUT2+SUPPLYINPUT3+SUPPLYINPUT4)/4,\n         REVERSE_DECISION = (REVERSE_DECISION1+REVERSE_DECISION2+REVERSE_DECISION3+REVERSE_DECISION4)/4)\n\nm&lt;-df %&gt;% select(contains(c(\"SAFETY\",\n                            \"OFFER\",\n                            \"SUPPLY\",\n                            \"AWARE\",\n                            \"REVERSE\")))\n\ndf_new&lt;-df_new %&gt;% \n  select(-names(m))\n\n  \n##Convert categorical cols in df_new to factor class:\ncols &lt;- c(names(df %&gt;%  select (-c(names(m)))))\ndf_new[cols]&lt;-lapply(df_new[cols],factor)\n\n##Check again:\nglimpse(df_new)\n\n\nRows: 208\nColumns: 13\n$ SEX              &lt;fct&gt; Nam, Nữ, Nam, Nam, Nữ, Nam, Nữ, Nam, Nữ, Nữ, Nữ, Nam,…\n$ AGE              &lt;fct&gt; Từ 19 đến 30 tuổi, Từ 19 đến 30 tuổi, Từ 19 đến 30 tu…\n$ MAJOR            &lt;fct&gt; Trí thức, Học sinh - Sinh viên, Học sinh - Sinh viên,…\n$ STATUS           &lt;fct&gt; Đã có gia đình, Độc thân, Độc thân, Đang có người yêu…\n$ RETURN_EW        &lt;fct&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ PRE_METHOD       &lt;fct&gt; Vứt rác, Bán ve chai, Vứt rác, Bán ve chai, Bán ve ch…\n$ JOIN_HAND        &lt;fct&gt; 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,…\n$ PRIOR            &lt;fct&gt; Hoàn trả doanh nghiệp, Hoàn trả doanh nghiệp, Không b…\n$ SAFETY           &lt;dbl&gt; 4.00, 3.75, 3.75, 3.25, 3.00, 5.00, 4.00, 4.00, 3.50,…\n$ OFFER            &lt;dbl&gt; 3.00, 3.50, 3.25, 3.75, 3.00, 5.00, 4.00, 4.00, 4.00,…\n$ AWARENESS        &lt;dbl&gt; 3.00, 3.75, 3.25, 3.50, 3.00, 5.00, 3.50, 4.00, 4.75,…\n$ SUPPLYINPUT      &lt;dbl&gt; 3.00, 4.50, 3.75, 4.00, 3.00, 5.00, 3.25, 4.00, 4.75,…\n$ REVERSE_DECISION &lt;dbl&gt; 3.00, 2.50, 3.25, 2.75, 3.00, 5.00, 4.00, 4.00, 4.75,…\n\n\n\n\nOverall, Likert scales are a popular and useful tool for capturing subjective information, though the ordinal nature of the data requires careful consideration in analysis and interpretation.\n\n\n\n\n\n\n\n\nIn other hand, we can use a distribution plot of Likert data visualizes how responses are distributed across the different scale points. Given the ordinal nature of Likert data, such a plot helps to understand the frequency or proportion of responses for each category on the scale.\nSteps to Create a Distribution Plot for Likert Data:\n\nCollect Data: Gather your Likert scale responses from the survey or questionnaire.\nTabulate Responses: Count the number of responses for each Likert scale point (e.g., “Strongly Disagree” to “Strongly Agree”).\nChoose a Visualization Tool: You can use various tools like Excel, Google Sheets, or statistical software (e.g., R, Python’s Matplotlib/Seaborn) to create the plot.\nPlot the Data: you can consider between Bar Chart - where each bar represents the frequency or percentage of responses for each scale point or Pie Chart which shows the proportion of responses for each scale point. If you are dealing with multiple groups or categories, Stacked Bar Chart maybe useful where bars are segmented to show the distribution across the Likert scale for each group.\n\n\n\n\n\n\n\n\n\nA correlation test is used to determine the strength and direction of the relationship between two continuous variables. It assesses whether changes in one variable are associated with changes in another variable."
  },
  {
    "objectID": "NCKH_final.html#mediation-analyst-approach",
    "href": "NCKH_final.html#mediation-analyst-approach",
    "title": "Introduction",
    "section": "2.1 Mediation analyst approach:",
    "text": "2.1 Mediation analyst approach:\nA mediation model examines whether the effect of an independent variable (X) on a dependent variable (Y) is transmitted through a third variable, called a mediator (M). For the mediation analysis to be valid and reliable, certain assumptions must be met. Here are the key assumptions:\nSteps in Mediation Analysis:\n\nTest the Total Effect:\n\nExamine the direct relationship between the independent variable (X) and the dependent variable (Y).\n\nTest the Path from X to M:\n\nAnalyze the relationship between the independent variable (X) and the mediator (M).\n\nTest the Path from M to Y:\n\nAssess the relationship between the mediator (M) and the dependent variable (Y), controlling for the independent variable (X).\n\nTest the Direct Effect:\n\nExamine the direct effect of the independent variable (X) on the dependent variable (Y) while controlling for the mediator (M).\n\nCompare Effects:\n\nDetermine if the mediation effect (indirect effect) is significant and if it explains the relationship between X and Y beyond the direct effect.\n\n\nStatistical Methods:\n\nBaron and Kenny’s Approach: Traditional method involving sequential regression analyses to test the above paths.\nBootstrapping: A resampling technique used to estimate the confidence intervals of the indirect effect, often considered more robust than Baron and Kenny’s approach.\nStructural Equation Modeling (SEM): A comprehensive method that can simultaneously estimate multiple mediation paths and their relationships.\n\nBy meeting these assumptions and carefully testing the mediation model, researchers can better understand the mechanisms through which an independent variable influences a dependent variable, providing deeper insights into the underlying processes.\n\n2.1.1 Check assumptions before modeling:\nAssumptions of a Mediation Model:\n\nLinearity:\n\nThe relationships among the independent variable (X), mediator (M), and dependent variable (Y) should be linear. This means that the effects are additive and can be represented by straight lines in a scatterplot.\n\nIndependence of Errors:\n\nThe residuals or errors in the regression equations should be independent of each other. This assumption ensures that the model is not influenced by correlated errors between observations.\n\nHomoscedasticity:\n\nThe variance of the residuals should be constant across all levels of the independent and mediator variables. In other words, the spread of errors should be similar across all values of X and M.\n\nNormality of Errors:\n\nThe residuals or errors in the regression models should be normally distributed. This assumption helps in making valid inferences about the model parameters and their statistical significance.\n\nNo Multicollinearity:\n\nThere should be no high correlation among the independent variables, including the mediator variable, as high multicollinearity can make it difficult to isolate the individual effects of each variable.\n\nMeasurement Validity:\n\nThe variables should be measured accurately. This means the mediator (M) should genuinely capture the concept it is supposed to represent, and there should be reliable and valid measures for X, M, and Y.\n\nTemporal Ordering:\n\nFor causal inferences, it is crucial that the independent variable (X) precedes the mediator (M), which in turn precedes the dependent variable (Y). This temporal ordering ensures that changes in X can affect M, which then influences Y."
  },
  {
    "objectID": "NCKH_final.html#implementing-in-r",
    "href": "NCKH_final.html#implementing-in-r",
    "title": "Introduction",
    "section": "2.2 Implementing in R:",
    "text": "2.2 Implementing in R:\n\n\nCode\n## Using the {mvnormalTest} package for univariate (Shapiro-Wilk’s W) and multivariate normality (Mardia’s Multivariate Skewness and Kurtosis tests).\nlibrary(mvnormalTest)\ncheck&lt;-mardia(df_new %&gt;% select(c(\"SAFETY\",\n                                  \"OFFER\",\n                                  \"AWARENESS\",\n                                  \"SUPPLYINPUT\",\n                                  \"REVERSE_DECISION\")))\n\n## Check the Univariate normality test\ncheck$uv.shapiro\n## Check the Multivariate normaility test\ncheck$mv.test\n\n\n\n\n                 W      p-value UV.Normality\nSAFETY           0.8643 0       No          \nOFFER            0.9484 0       No          \nAWARENESS        0.8844 0       No          \nSUPPLYINPUT      0.8857 0       No          \nREVERSE_DECISION 0.9473 0       No          \n\n\n          Test Statistic p-value Result\n1     Skewness  221.3551       0     NO\n2     Kurtosis   12.3111       0     NO\n3 MV Normality      &lt;NA&gt;    &lt;NA&gt;     NO\n\n\n\n\n\n2.2.1 Modeling with {lavaan} package:\nThe lavaan package in R is a powerful tool for structural equation modeling (SEM). It allows users to specify, estimate, and evaluate complex statistical models that include latent variables, measurement models, and path models. Here’s an overview of the lavaan package and its key functionalities:\nAbout model Structural Equation Modeling (SEM) in lavaan, it will estimate:\n\nConfirmatory Factor Analysis (CFA): Tests how well a set of observed variables represents underlying latent constructs.\nPath Analysis: Examines direct and indirect relationships among variables.\nLatent Variable Models: Models that include unobserved (latent) variables alongside observed variables.\nMixture Models: Handles latent variable models with multiple subpopulations.\n\nBeside, the estimated algorithm, lavaan use 3 methods including Maximum Likelihood (ML), Generalized Least Squares (GLS), and Robust Maximum Likelihood (MLR).\n\n\n\n\n\n\nTips for you !!!\n\n\n\nYou can mention it by argument estimator in function lavaan::sem.\n\n\n\n\nCode\n##Define the model:\nlibrary(lavaan)\nmediation_model &lt;- '\n  # Direct effects\n  AWARENESS ~ a*SAFETY  +  b*OFFER + c*SUPPLYINPUT\n  REVERSE_DECISION ~ d*AWARENESS + e*SAFETY + f*OFFER\n\n  # Indirect effect:\n  indirect := d*(a + b + c)\n\n  # Total effect:\n  total := e + f + indirect'\n\n##Estimate the mediation model\nreg3 &lt;- sem(mediation_model,estimator = \"ML\",data = df_new)\n##Summarize the results\nsummary(reg3, \n        standardized = TRUE, \n        fit.measures = TRUE)\n\n\nlavaan 0.6.14 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         8\n\n  Number of observations                           208\n\nModel Test User Model:\n                                                      \n  Test statistic                                 2.839\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.092\n\nModel Test Baseline Model:\n\n  Test statistic                               452.569\n  Degrees of freedom                                 7\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.996\n  Tucker-Lewis Index (TLI)                       0.971\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)               -345.801\n  Loglikelihood unrestricted model (H1)       -344.381\n                                                      \n  Akaike (AIC)                                 707.601\n  Bayesian (BIC)                               734.302\n  Sample-size adjusted Bayesian (SABIC)        708.954\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.094\n  90 Percent confidence interval - lower         0.000\n  90 Percent confidence interval - upper         0.231\n  P-value H_0: RMSEA &lt;= 0.050                    0.176\n  P-value H_0: RMSEA &gt;= 0.080                    0.700\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.011\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                     Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  AWARENESS ~                                                             \n    SAFETY     (a)      0.153    0.048    3.197    0.001    0.153    0.154\n    OFFER      (b)      0.185    0.042    4.401    0.000    0.185    0.186\n    SUPPLYINPU (c)      0.614    0.047   12.967    0.000    0.614    0.636\n  REVERSE_DECISION ~                                                      \n    AWARENESS  (d)      0.458    0.077    5.922    0.000    0.458    0.435\n    SAFETY     (e)      0.218    0.071    3.068    0.002    0.218    0.208\n    OFFER      (f)      0.176    0.066    2.668    0.008    0.176    0.168\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .AWARENESS         0.206    0.020   10.198    0.000    0.206    0.238\n   .REVERSE_DECISI    0.463    0.045   10.198    0.000    0.463    0.484\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n    indirect          0.436    0.076    5.754    0.000    0.436    0.425\n    total             0.830    0.061   13.658    0.000    0.830    0.802\n\n\nCode\n##Plot the result:\nlabel1&lt;-list(SAFETY = \"Sự an toàn\",\n             OFFER = \"Sự ưu đãi\",\n             SUPPLYINPUT = \"Cung cấp nguyên liệu đầu vào\",\n             AWARENESS = \"Nhận thức xã hội\",\n             REVERSE_DECISION = \"Quyết định tham gia Logistics ngược\")\n\nlavaanPlot::lavaanPlot(model = reg3,\n                       labels = label1, \n                       node_options = list(shape = \"box\", fontname = \"Helvetica\"),\n                       edge_options = list(color = \"grey\"),\n                       coefs = TRUE)\n\n\n\n\n\n\n\n\n2.2.2 Modeling with {mediation} package:\nThe mediation package in R is designed to facilitate the analysis of mediation models. Mediation models are used to understand the process through which an independent variable (IV) influences a dependent variable (DV) via one or more mediator variables. Here’s a brief summary of what the package offers:\n\nCore Functionality: The primary function of the mediation package is mediate(), which estimates the causal mediation effects and provides statistical tests to evaluate the significance of these effects.\nComponents:\n\nModel Specification: You need to fit two models—one for the mediator and one for the outcome variable. The mediation package uses these models to estimate the direct and indirect effects.\nBootstrapping: The package supports bootstrapping methods to compute confidence intervals for the indirect effects, enhancing robustness and reliability.\n\nKey Features:\n\nEstimation of Effects: Provides estimates for direct, indirect, and total effects, allowing for a comprehensive understanding of mediation.\nSensitivity Analysis: Offers tools to assess the sensitivity of the mediation effect to potential violations of assumptions.\nMultiple Mediators: Supports models with multiple mediators, including interactions among them.\n\nOutput:\n\nSummary Tables: Produces tables summarizing the estimated effects and their significance.\nVisualization: Tools to visualize mediation effects, helping to interpret and present findings.\n\nUsage:\n\nYou typically fit a regression model for the mediator and another for the outcome, then use mediate() to analyze the mediation process.\n\n\n\n\nCode\n##Or we can use Preacher & Hayes (2004) approach:\n###Using meidation package:\nlibrary(mediation)\nfitM &lt;- lm(AWARENESS ~ SUPPLYINPUT + SAFETY + OFFER, \n           data=df_new) \nfitY &lt;- lm(REVERSE_DECISION ~ AWARENESS + SAFETY + OFFER, \n           data=df_new)\nfitMed &lt;- mediate(fitM, \n                  fitY,\n                  sims = 1000,\n                  boot = TRUE,\n                  treat = \"SAFETY\",\n                  mediator=\"AWARENESS\")\nsummary(fitMed)\n\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.0700       0.0187         0.13   0.004 ** \nADE              0.2178       0.0720         0.39   0.006 ** \nTotal Effect     0.2878       0.1461         0.44  &lt;2e-16 ***\nProp. Mediated   0.2432       0.0606         0.58   0.004 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 208 \n\n\nSimulations: 1000 \n\n\nCode\nplot(fitMed)\n\n\n\n\n\nThe mediate function gives us:\n\nAverage Direct Effects (ADE) means direct effect of SAFETY and OFFER on REVERSE_DECISION without AWARENESS.\nCombined indirect and direct effects (Total Effect) means total effect of SAFETY and OFFER on REVERSE_DECISION plus the indirect effect of AWARENESS.\nAverage Causal Mediation Effects (ACME) = Total Effect - ADE\nThe ratio of these estimates (Prop. Mediated).\n\nThe ACME here is the indirect effect of M (total effect - direct effect) and thus this value tells us if our mediation effect is significant (Alyssa Blair, n.d.).\nIn summary, the mediation package in R is a robust tool for analyzing and interpreting the pathways through which variables influence each other, providing both statistical estimates and tools for visualization and sensitivity analysis.\n\n\n2.2.3 Check assumptions after modeling:\nThe gvlma function in R is part of the gvlma package, which stands for “Global Validation of Linear Models Assumptions.” This function is used to perform a comprehensive diagnostic check of the assumptions underlying linear regression models. The gvlma package evaluates several key assumptions to ensure the validity of a linear regression model.\nA lot of test gvlma can do such as:\n\nLinearity: Checks if the relationship between predictors and the outcome is linear.\nHomoscedasticity: Assesses if the residuals have constant variance.\nIndependence: Tests if the residuals are independent.\nNormality of Residuals: Checks if residuals are normally distributed.\nModel Specification: Evaluates whether the model is correctly specified.\n\n\n\nCode\nlibrary(gvlma)\ngvlma(fitM)\n\n\n\nCall:\nlm(formula = AWARENESS ~ SUPPLYINPUT + SAFETY + OFFER, data = df_new)\n\nCoefficients:\n(Intercept)  SUPPLYINPUT       SAFETY        OFFER  \n     0.2243       0.6135       0.1528       0.1852  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitM) \n\n                       Value p-value                Decision\nGlobal Stat        4.2262378 0.37625 Assumptions acceptable.\nSkewness           0.0004398 0.98327 Assumptions acceptable.\nKurtosis           2.7098541 0.09973 Assumptions acceptable.\nLink Function      1.2198735 0.26939 Assumptions acceptable.\nHeteroscedasticity 0.2960704 0.58636 Assumptions acceptable.\n\n\nCode\ngvlma(fitY)\n\n\n\nCall:\nlm(formula = REVERSE_DECISION ~ AWARENESS + SAFETY + OFFER, data = df_new)\n\nCoefficients:\n(Intercept)    AWARENESS       SAFETY        OFFER  \n     0.2296       0.4580       0.2178       0.1760  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fitY) \n\n                       Value   p-value                   Decision\nGlobal Stat        1.271e+02 0.000e+00 Assumptions NOT satisfied!\nSkewness           5.919e+01 1.432e-14 Assumptions NOT satisfied!\nKurtosis           6.787e+01 2.220e-16 Assumptions NOT satisfied!\nLink Function      8.530e-04 9.767e-01    Assumptions acceptable.\nHeteroscedasticity 2.851e-02 8.659e-01    Assumptions acceptable."
  }
]